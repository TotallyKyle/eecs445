# -*- coding: utf-8 -*-
""" 
Example of use multi-layer perceptron
=====================================

Task: Approximation function: 1/2 * sin(x)

"""

import neurolab as nl
import numpy as np
import InputModels as model

# Create train samples
x = np.linspace(-7, 7, 20)
y = np.sin(x) * 0.5

#size = len(x)
#inp = x.reshape(size,1)
#tar = y.reshape(size,1)

x = [[0,0], [1,1],[2,2],[3,3],[4,4]]
test = [[5,5]]
y = [0,2,4,6,8]
size = len(y)
inp = np.array(x)
tar = np.array(y)
tar = tar.reshape(size,1)

#inp = x.reshape(size,1)
#tar = y.reshape(size,1)

min_val = 0
max_val = 4
# Create network with 2 layers and random initialized
#the first list refers to the range values for each input, 
#the length of the second list is the # of layers and each number is the # of nerons
#net = nl.net.newff([[-0.5, 0.5], [-0.5, 0.5], [-1,10]], [5, 3, 1])
net = nl.net.newff([[0,4],[0,4]],[5, 1])

# Train network
error = net.train(inp, tar, epochs=500, show=100, goal=0.02)

# Simulate network
out = net.sim(inp)

# # Plot result
# import pylab as pl
# pl.subplot(211)
# pl.plot(error)
# pl.xlabel('Epoch number')
# pl.ylabel('error (default SSE)')

# x2 = np.linspace(-6.0,6.0,150)
# y2 = net.sim(x2.reshape(x2.size,1)).reshape(x2.size)

# y3 = out.reshape(size)

# pl.subplot(212)
# pl.plot(x2, y2, '-',x , y, '.', x, y3, 'p')
# pl.legend(['train target', 'net output'])
# pl.show()