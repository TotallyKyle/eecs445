# -*- coding: utf-8 -*-
""" 
Example of use multi-layer perceptron
=====================================

Task: Approximation function: 1/2 * sin(x)

"""

import neurolab as nl
import numpy as np
import InputModels as model
import FeatureParser as parser
import random
from scipy.cluster.vq import kmeans,vq

# Create train samples, in this case the input data model
# has 5 features, to predict the exchange rate on day N
# the feature is the exchange rate on day N-1 ... N-5
# data is a tuple where 
# data[0] = input, data[1] = output
# data[2] = min_val, data[3] = max_val in this sample model
data = []
feature_value_range = {}
joined_data = []
joined_data = parser.add_feature_to_data_alt(data, 'initial_features_edited.csv', 'DJIA USA')
data = joined_data[0]
feature_value_range['DJIA USA'] = joined_data[1]
joined_data = parser.add_feature_to_data_alt(data, 'initial_features_edited.csv', 'NYK')
data = joined_data[0]
feature_value_range['NYK'] = joined_data[1]
# joined_data = parser.add_feature_to_data(data, 'USDJPY_complete.csv', 'High')
# data = joined_data[0]
# feature_value_range['High'] = joined_data[1]
# joined_data = parser.add_feature_to_data(data, 'USDJPY_complete.csv', 'Volume')
# data = joined_data[0]
# feature_value_range['Volume'] = joined_data[1]


#add time series features
timeSeries = model.timeDelayedFeature(5)
timeSeriesFeature = timeSeries[0]
feature_value_range['0'] = timeSeries[1]
feature_value_range['1'] = timeSeries[1]
feature_value_range['2'] = timeSeries[1]
feature_value_range['3'] = timeSeries[1]
feature_value_range['4'] = timeSeries[1]

data = parser.join_on_minimum(data, timeSeriesFeature)

# construct the target value vector
target = []
target_val = parser.add_feature_to_data(target, 'USDJPY_complete.csv', 'High')
target = parser.match_target_to_data(target_val[0], data)


# this converts the data structure from list of dicts to list of lists
# for both input and target
target = parser.convert_input(target)
target = [val for sublist in target for val in sublist]
target = model.normalize_target(target, target_val[1][0], target_val[1][1])

feature_value_range = parser.convert_feature_value_range(feature_value_range);
data = parser.convert_input(data)

minVal = min(target)
maxVal = max(target)
# we still need to implement validation so 0% is used for validation
# 70% of the data is used for training and 30% for testing here
segmented_input = parser.segmentation(data, 0.7, 0, 0.3)
segmented_target = parser.segmentation(target, 0.7, 0, 0.3)
train_input = segmented_input[0]
train_target = segmented_target[0]

test_input = segmented_input[2]
test_target = segmented_target[2]

#COMBINE TRAIN INPUT AND TARGET
#hybrid_train = train_input;
#for i in range(len(train_input)):
#	hybrid_train[i].append(train_target[i])
	
train_size = len(train_input)
test_size = len(test_input)
# we need to convert the format of the data to be
# compliant with the neurolab API, print out the
# values of inp and tar to see format
train_input = np.array(train_input)
train_target = np.array(train_target)
train_target = train_target.reshape(train_size, 1)

# computing K-Means with K = 2 (2 clusters)
init_centroids = []
num_cluster = 2
for i in range(num_cluster):
	idx = random.randint(0,len(train_input)-1)
	init_centroids.append(train_input[idx])
init_centroids = np.array(init_centroids)
centroids,_ = kmeans(train_input, init_centroids)
# assign each sample to a cluster
idx,_ = vq(train_input,centroids)

# Create network with 3 layers with 5, 5, and 1 neruon(s) in each layer 
# and randomly initialized
# the first list refers to the range of values for each input feature, 
# the length of the second list is the # of layers and each number is the # of nerons
# EX: net = nl.net.newff([[-0.5, 0.5], [-0.5, 0.5], [-1,10]], [5, 3, 1])
nets = []
for i in range(num_cluster):
	nets.append(nl.net.newff(feature_value_range,[9, 5, 1]))
	nl.init.midpoint(nets[i].layers[1])
	# Train network
	error = nets[i].train(train_input[idx==i], train_target[idx==i], epochs=500, show=100, goal=0.02)


# Simulate network on test data
test_input = np.array(test_input)
test_target = np.array(test_target)
test_target = test_target.reshape(len(test_target), 1)
testIdx,_ = vq(test_input,centroids)
cluster_input = []
cluster_target = []
cluster_size = []
cluster_min = []
cluster_max = []
cluster_prediction = []

cidx = 0
for i in range(num_cluster):
	if(len(test_input[testIdx==i]) > 0):
		cluster_input.append(test_input[testIdx==i])
		cluster_target.append(test_target[testIdx==i])
		cluster_size.append(len(cluster_input[cidx]))
		cluster_min.append(min(cluster_target[cidx]))
		cluster_max.append(max(cluster_target[cidx]))
		cluster_prediction.append(model.denormalize_target(nets[i].sim(cluster_input[i]).reshape(1,cluster_size[i]).tolist()[0],cluster_min[i], cluster_max[i]))
		cidx = cidx + 1


# Plot result
import pylab as pl
# pl.subplot(211)
# pl.plot(error)
# pl.xlabel('Epoch number')
# pl.ylabel('error (default SSE)')

# x2 = np.linspace(-6.0,6.0,150)
# y2 = net.sim(x2.reshape(x2.size,1)).reshape(x2.size)

# y3 = out.reshape(size)

# pl.subplot(212)


if len(cluster_target) > 0:
	pl.subplot(411)
	pl.plot(range(cluster_size[0]), cluster_prediction[0], '-',range(cluster_size[0]) , model.denormalize_target(cluster_target[0], cluster_min[0], cluster_max[0]), '.')
	pl.legend(['prediction value', 'actual value'])
	pl.title('cluster 0')

if len(cluster_target) > 1:
	pl.subplot(412)
	pl.plot(range(cluster_size[1]), cluster_prediction[1], '-',range(cluster_size[1]) , model.denormalize_target(cluster_target[1], cluster_min[1], cluster_max[1]), '.')
	pl.legend(['prediction value', 'actual value'])
	pl.title('cluster 1')

if len(cluster_target) > 2:
	pl.subplot(421)
	pl.plot(range(cluster_size[2]), cluster_prediction[2], '-',range(cluster_size[2]) , model.denormalize_target(cluster_target[2], cluster_min[2], cluster_max[2]), '.')
	pl.legend(['prediction value', 'actual value'])
	pl.title('cluster 2')

if len(cluster_target) > 3:
	pl.subplot(422)
	pl.plot(range(cluster_size[3]), cluster_prediction[3], '-',range(cluster_size[3]) , model.denormalize_target(cluster_target[3], cluster_min[3], cluster_max[3]), '.')
	pl.legend(['prediction value', 'actual value'])
	pl.title('cluster 3')

pl.show()